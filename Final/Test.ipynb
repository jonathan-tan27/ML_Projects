train_loss_arr = []
train_acc_arr = []
train_tpr_arr = []
train_tnr_arr = []

dev_loss_arr = []
dev_acc_arr = []
dev_tpr_arr = []
dev_tnr_arr = []

# Loop over the dataset multiple times
for epoch in range(num_epochs):
    # Keep track of training loss
    train_loss = 0.0
    # Keep track of dev loss
    dev_loss = 0.0
    # Keep track of accuracy measurements
    train_loss_plot = []
    dev_loss_plot = []
    acc, tpr, tnr = 0.0, 0.0, 0.0

    # Train the model
    start_time = time.time()
    model.train()
    for batch_idx, (file, image, label) in enumerate(train_loader):
        if USE_GPU:
            data, target = image.cuda(), label.cuda()
        else:
            data, target = image, label
        # Zero the parameter gradients
        optimizer.zero_grad()
        # Forward pass
        output = model(data)
        # Update target to be the same dimensions as output
        target = target.view(output.shape[0], 1).float()
        # Get accuracy measurements
        acc, tpr, tnr = training_accuracy(output, target, batch_idx, acc, tpr, tnr)
        # Calculate the batch's loss
        curr_train_loss = criterion(output, target)
        # Backward pass
        curr_train_loss.backward()
        # Save loss for loss plot
        train_loss_plot.append(curr_train_loss.item())
        # Perform a single optimization step to update parameters
        optimizer.step()
        # Update the training loss
        train_loss += curr_train_loss.item()
        # Print debug info every 32 batches
        if (batch_idx) % 32 == 0:
            print('Epoch {}/{}; Iter {}/{}; Loss: {:.4f}; Acc: {:.3f}; True Pos: {:.3f}; True Neg: {:.3f}'
                   .format(epoch+1, num_epochs, batch_idx + 1, len(train_loader), curr_train_loss.item(), acc, tpr, tnr))
    end_time = time.time()
    
    # Evaluate the model
    model.eval()
    with torch.no_grad():
        for batch_idx, (file, image, label) in enumerate(dev_loader):
            if USE_GPU:
                data, target = image.cuda(), label.cuda()
            else:
                data, target = image, label
            # Get predicted output
            output = model(data)
            # Update target to be the same dimensions as output
            target = target.view(output.shape[0], 1).float()
            # Get accuracy measurements
            dev_acc, dev_tpr, dev_tnr = dev_accuracy(output, target)
            # Calculate the batch's loss
            curr_dev_loss = criterion(output, target)
            # Save loss for loss plot
            dev_loss_plot.append(curr_dev_loss.item())
            # Update the dev loss
            dev_loss += curr_dev_loss.item()
    
    # Update dev loss arrays
    dev_loss_arr.append(np.mean(np.array(dev_loss_plot)))
    dev_acc_arr.append(dev_acc)
    dev_tpr_arr.append(dev_tpr)
    dev_tnr_arr.append(dev_tnr)

    # Update training loss arrays
    train_loss_arr.append(np.mean(np.array(train_loss_plot)))
    train_acc_arr.append(acc)
    train_tpr_arr.append(tpr)
    train_tnr_arr.append(tnr)
    
    # Calculate average loss
    train_loss = train_loss/len(train_loader.dataset)
    dev_loss = dev_loss/len(dev_loader.dataset)

    print('Epoch {}/{}; Loss: {:.4f}; Train Acc: {:.3f}; Train TPR: {:.3f}; Train TNR: {:.3f}; Epoch Time: {} mins; \nDev Loss: {:.4f}; Dev Acc: {:.3f}; Dev TPR: {:.3f}; Dev TNR: {:.3f}\n'
        .format(epoch+1, num_epochs, train_loss, acc, tpr, tnr, round((end_time - start_time)/ 60., 2), dev_loss, dev_acc, dev_tpr, dev_tnr))
    
    if dev_loss < dev_loss_min:
        print('Dev loss decreased ({:.6f} --> {:.6f}).  Saving model ...'
              .format(dev_loss_min, dev_loss))
        torch.save(model.state_dict(), 'model_v2.pt')
        state = fetch_state(epoch = epoch + 1, model = model, optimizer = optimizer)
        if (dev_acc >= dev_max_acc):
            save_checkpoint(state = state, is_best = True)
            dev_max_acc = dev_acc
        else:
            save_checkpoint(state = state, is_best = False)
        dev_loss_min = dev_loss
        bad_epoch_count = 0
    # If dev loss didn't improve, increase bad_epoch_count and stop if
    # bad_epoch_count >= early_stop_limit (early stop)
    else:
        bad_epoch_count += 1
        print('{} epochs of increasing dev loss ({:.6f} --> {:.6f}).'
              .format(bad_epoch_count, dev_loss_min, dev_loss))
        if (bad_epoch_count >= early_stop_limit):
            print('Stopping training')
            stop = True
            
    # Save model if train loss has decreased
    ### Focused on train loss at the moment because we want to overfit ###
    ### the training data before worrying about our variance.          ###
    #if train_loss <= train_loss_min:
    #    print('Train loss decreased ({:.6f} --> {:.6f}).  Saving model ...'
    #          .format(train_loss_min, train_loss))
    #    torch.save(model.state_dict(), 'model_v1.pt')
    #    train_loss_min = train_loss
    #    bad_epoch_count = 0
    ## If train loss didn't improve, increase bad_epoch_count and stop if
    ## bad_epoch_count >= early_stop_limit (early stop)
    #else:
    #    bad_epoch_count += 1
    #    print('{} epochs of increasing training loss'.format(bad_epoch_count))
    #    if (bad_epoch_count >= early_stop_limit):
    #        print('Stopping training')
    #        stop = True
        
    if (stop):
        break